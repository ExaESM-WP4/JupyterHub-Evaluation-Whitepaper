\section{Jupyter at DKRZ}
\label{sect:jupyter-at-dkrz}

In the following, the Jupyter-based solutions provided by DKRZ\footnote{Deutsches Klimarechenzentrum} are investigated.
First, a brief overview on the available compute infrastructure is given, then the officially supported Jupyter-based access options are described.
The goal is to enable a comprehensive discussion of the JupyterHub@JSC from a more general Jupyter@HPC viewpoint.

\subsection{HPC system infrastructure}

DKRZ operates \href{https://www.dkrz.de/up/systems/mistral}{Mistral} (HLRE-3) which is a tier-2 HPC system \cite{Wissenschaftsrat2015, GaussAllianz2020} with Earth system researchers as target user group.
The system was originally installed in July 2015\footnote{https://www.dkrz.de/up/systems/mistral/configuration} and is planned to operate until mid-2021\footnote{https://www.dkrz.de/kommunikation/aktuelles/dkrz-verfuenffacht-supercomputing-leistung-mit-neuem-bullsequana-von-atos}.
The system currently consists of roughly 1600 Intel Xeon E5-2680v3 phase-1 and 1800 Intel Xeon E5-2695V4 phase-2 compute nodes.
A very small fraction, i.e. 21 compute nodes (about 0.6\% of the total system, available via the \verb|gpu| partition) additionally operates a pair of several different types of Nvidia Tesla GPUs\footnote{what types of GPUs?}.
The main part of the system, i.e. about 96.2\% of the HPC system resources are reserved for classic multi- and full-node batch compute tasks (\verb|compute| and \verb|compute2| partition), with only a very small fraction of 1.1\% being reserved for shared-node compute tasks (\verb|shared| partition), respectively.
Another about 1.3\% of the system is explicitely reserved for data processing tasks (\verb|prepost| partition), whereas the remaining about 0.9\% are reserved for XXX tasks (\verb|miklip| partition)\footnote{are these dedicated for a special compute project?}.
The system is supplemented by seven login nodes which are dedicated for file editing, source code compilation, and the preparation and monitoring of batch tasks\footnote{https://www.dkrz.de/up/systems/mistral/login-and-environment} only.
Interactive command line analysis tasks are supposed to happen on the five \verb|mistralpp| interactive nodes\footnote{add URL}, that can also directly be accessed via external SSH sessions.

\subsection{Documentation from user perspective}

Documentation about how to do general interactive and batch-type data analysis/processing tasks are provided only on and are easily found via the DKRZ user portal \footnote{https://www.dkrz.de/up/services/analysis/data-processing/processing-on-mistral}.
The documentation about Jupyter is also centrally managed and easy to find on the user portal\footnote{}.
There is, however, no obvious way of getting to the documentation from the actual JupyterHub login page and/or control panel.

The DKRZ documents two supported ways of working with Jupyter on Mistral.
They offer a short "how to" and a bash script that allows starting standalone Jupyter instances on compute nodes and they offer a JupyterHub service.
Both ways of working with Jupyter are documented separately.
The documentation of the script approach lives in the Mistral specific documentation\footnote{https://www.dkrz.de/up/systems/mistral/programming/jupyter-notebook}, while the JupyterHub is documented as a separate service\footnote{https://www.dkrz.de/up/systems/jupyterhub-dkrz.de-1}.

The JupyterHub documentation does not discriminate between differente experience levels, but is very concise and makes it easy to find relevant information for all Jupyter-experience levels of users.
There is a walkthrough documentation for beginning users demonstrates every step from login to actual work on the HPC cluster, including recommendations about which of the available job profiles (see section \ref{} XXX below) are suitable for which task.
It especially highlights machine specific and SLURM scheduler specific background information that is helpful for unexperienced users and for experienced users not familar with HPC systems.
Where possible, links to the community documentation of Jupyter are given\footnote{as the link to the community documentation points to the latest version, there is a possible source of inconsistency/confusion, though}.
A target user group specific example Jupyter notebook is linked and available via the DKRZ Gitlab server.

The documentation seems to be in good agreement with the actually deployed Jupyter service.
It should be noted though that the deployed Jupyter version is approximately two years old\footnote{version 0.7.2 released in XXX}.

The given examples seem to match the actually deployed Jupyter service.
There doesn't seem to have been a lot of changes to the JupyterHub system since its initial deployment, thus it's not possible to say how the documentation of changes to the JupyterHub system is done/solved.
Currently, there is no explicit version information on both the documentation and the examples.

\subsection{JupyterHub service}

The \href{https://jupyterhub.dkrz.de/}{JupyterHub} service\footnote{currently running on JupyterHub version 0.7.2} at DKRZ has been set up in September 2017\footnote{as taken from the package folder setup dates in the {\tt jupyterhub/.0.1} module} and is currently operated what DKRZ calls an extended testing phase\footnote{\url{https://www.dkrz.de/up/de-systems/de-jupyterhub-dkrz.de-1/de-jupyterhub-dkrz.de} and \url{https://www.dkrz.de/up/systems/dkrz-extended-test-phase}}.

Access to the JupyterHub service at DKRZ is automatically given for any user registered via the DKRZ web interface\footnote{https://luv.dkrz.de/projects/newuser/} but Jupyter notebooks can only be spawned with a valid compute project membership\footnote{https://www.dkrz.de/up/systems/jupyterhub-dkrz.de-1}.

The DKRZ JupyterHub service allows to manage/connect exactly one Jupyter notebook server on the Mistral batch partitions, which is mainly due to the limited functionality of the JupyterHub version\footnote{access management for named servers was added only in version 1.0.0, according to the changelogs provided at https://jupyterhub.readthedocs.io/en/stable/changelog.html#id3} that is deployed.

The Jupyter notebook servers are set up based on pre-defined job profiles from a drop-down menu, the compute project identifier for possible node hour claims has to be inserted manually by the user.
Jupyter notebooks can not be spawned on each part of Mistral's hardware, i.e. only on the \verb|shared|, \verb|prepost| and \verb|compute| partitions.
For the \verb|shared| partition a scientific user can currently request 1, 2 and 4 CPUs with 2.5, 5 and 10 GB of memory with a walltime limit of 12 hours, respectively.
On the \verb|prepost| partition the user can request the same CPU resources but with doubled memory, respectively.
A full node with 24 CPUs and 64 GB with a walltime limit of 8 hours can be requested from the \verb|prepost| partition, a full node with 256 GB of memory from the \verb|prepost| partition for 12 hours, respectively.
Access to the higher performance \verb|compute2| partition hardware and the GPU nodes available via the \verb|gpu| partition is currently not possible via the DKRZ JupyterHub.

\subsubsection{Jupyter working environment}

The default Jupyter server spawned by the DKRZ JupyterHub is the Jupyter notebook 5.1.0rc1 released in August 2017.
The Jupyter working environment is also defined in a (hidden) software environment module\footnote{at DKRZ http://modules.sourceforge.net/ is used} (see appendix \ref{app:dkrz-jupyter-env} for implementation details) on the HPC system.
Only Python kernels are available per default, and these are defined in the same environment as the Jupyter server.

\subsection{Jupyter remote control script}

At DKRZ a bash script\footnote{\url{https://gitlab.dkrz.de/k202009/ssh_scripts/-/blob/35560ae0c72d9d99e1448140ec0ac2210f7fd1e5/start-jupyter}} that allows users to remotely manage their Jupyter processes on Mistral is provided.
The script is executed on the user's personal computer and in a typical working session will
\begin{itemize}
    \item establish an SSH session to Mistral
    \item use a Jupyter environment definition stored on Mistral
    \item submit a Jupyter server either on mistralpp or as a batch job
    \item waits for the batch job to start (if necessary)
    \item setups an SSH tunnel
    \item opens the local web browser
    \item connects to remote Jupyter server via tunnel
\end{itemize}

The script is designed to work on Linux, Mac and Windows 10 systems.
While it is designed to provide a solution to setting up, connecting and terminating a remote Jupyter server, it doesn't provide a robust way of re-established a disrupted connection.
Jupyter servers need to be terminated manually, which might lead to orphaned Jupyter sessions both on mistralpp and the batch scheduler nodes.
The documentation highlights this problem for the batch scheduler nodes and gives an example on how to terminate an orphaned job\footnote{https://www.dkrz.de/up/systems/mistral/programming/jupyter-notebook}.
(Is this a problem also for mistralpp?)

Compared to the DKRZ JupyterHub the management of Jupyter working environments is very flexible, because the only requirement for the Jupyter server that is started on Mistral is that an access URL is printed to the processes' standard output.
In addition, the script can be modified and then allows for arbitrary resource requests on Mistral, i.e. everything that could be requested via the command line, and hence gives more flexibility than the pre-defined job profiles provided by the drop-down menu in their JupyterHub solution.
